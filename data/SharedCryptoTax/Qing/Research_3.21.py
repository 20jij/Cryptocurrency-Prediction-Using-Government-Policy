# -*- coding: utf-8 -*-
"""Research 2.28

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAcVDu2qtzIg_th0PhQutPtZE0NPSSUU

# Web Scraping
"""

# Commented out IPython magic to ensure Python compatibility.
import requests
from bs4 import BeautifulSoup
import pandas as pd

######1. Web Scraping all the tables about of the wiki page

#Make a request to the webpage
url = 'https://en.wikipedia.org/wiki/Legality_of_cryptocurrency_by_country_or_territory'
response = requests.get(url)
#Parse the HTML content
soup = BeautifulSoup(response.content, 'html.parser')
#Find all the tables in the webpage
tables = soup.find_all('table', {'class': 'wikitable'})
#Initialize an empty list to store the data
data = []
#Iterate through the tables
for table in tables:
    #Iterate through the rows of the table
    for row in table.find_all('tr'):
        cells = row.find_all('td')
        if len(cells) > 0:
            #Extract the information from each cell
            country = cells[0].text.strip()
            status = cells[1].text.strip()
            #Split the legal status by '\n'
            status_list = status.split('\n')
            legal_status = status_list[0]
            remaining_text = " ".join(status_list[1:])
            data.append([country, legal_status, remaining_text])
#Create a pandas DataFrame from the list
df = pd.DataFrame(data, columns=['Country', 'Legal Status','Remaining Text'])

#Get the reference number in the remaining text
from weakref import ref
import re
reference_number = []
#Get the reference number in the remaining text
for i in range(len(df)):
  num  = ' '.join(set(re.findall(r"\[(\d+)\]",df['Remaining Text'][i])))
  reference_number.append(num)
df['Reference number'] = reference_number
#Reset the index
df1 = df.set_index(['Country', 'Legal Status','Remaining Text']).apply(lambda x: x.str.split(' ').explode()).reset_index()

#Web scraping the citation number, citation link, and citation date
ref_section = soup.find_all("ol", class_="references")
references = ref_section[1].find_all('li')
#Save the reference id and reference link
id = []
links = []
dates = []
for i in range(0,len(references)):
  id.append(references[i].get('id')) # Get the citation id
  text = references[i].text
  ref_dates = []
  #Get the citation date
  date_match = re.findall(r'(?P<day>\d+)\s+(?P<month>\w+)\s+(?P<year>\d{4})', text)
  if date_match:
    for match in date_match:
        day = match[0]
        month = match[1]
        year = match[2]
        ref_dates.append(f"{day} {month} {year}")
  else:
    date_match = re.findall(r'(?P<month>\w+)\s+(?P<year>\d{4})', text)
    if date_match:
        for match in date_match:
            month = match[0]
            year = match[1]
            ref_dates.append(f"{month} {year}")
  if len(ref_dates) > 0:
    dates.append(ref_dates)
  else:
    year_match = re.findall(r'\b(\d{4})\b', text)
    if year_match:
        dates.append([year_match[0]])
    else:
        dates.append("NA") 
  try:
    links.append(references[i].find_all("a", class_="external text")[0].get('href')) # Get the citation link when class is external text
  except:
    links.append(references[i].find_all("a", class_="external free")[0].get('href')) # Get the citation link when class is external free
#Create a DataFrame from the list
df2 = pd.DataFrame({'Citation Id':id, 'Citation Link':links, 'Citation Date':dates })
#Get the citation number
citation_number = []
for i in range(len(df2)):
  id = re.findall(r'\b\d+\b', df2['Citation Id'][i])[-1]
  citation_number.append(id)
df2['Citation number'] = citation_number

#Merged the web scraping dataframe with the reference dataframe
merged = pd.merge(df1, df2, left_on='Reference number', right_on='Citation number', how ="outer")
merged.sort_values(by='Country', ascending=True, inplace=True)
merged.reset_index(drop=True, inplace=True)




######2. Web scraping the article contents

#Web scraping all the article contents
# %pip install newspaper3k
#Using newspaper3k to extract the articles of the links
from newspaper import Article
#Article extraction function
def extract_article_contents(url):
    article = Article(url)
    article.download()
    article.parse()
    return article.text
#Append the article contents to the dataframe
article_contents = []
for link in merged['Citation Link']:
    try:
      article_contents.append(extract_article_contents(link))
    except:
      try:
        from newspaper import Config
        user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
        config = Config()
        config.browser_user_agent = user_agent 
        article = Article(link, config=config)
        article.download()
        article.parse()
        article_contents.append(article.text)
      except:
        article_contents.append("No Link")

merged['Article Contents'] = article_contents
display(merged)

"""# Translate the article contents"""

!pip install googletrans==4.0.0-rc1

from googletrans import Translator

#Create a translator object
translator = Translator()

#Translate all the articles
article_contents_en = []
for article in merged['Article Contents']:
    try:
        article_en = translator.translate(article, dest='en').text
        article_contents_en.append(article_en)
    except:
        article_contents_en.append("Translation Unavailable")
#Add the translated articles to the DataFrame
merged['Article Contents (EN)'] = article_contents_en
display(merged)

"""# Word Clound

"""

# Import necessary libraries
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

# Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])

# filter the DataFrame to include only the rows where the legal status starts with "Legal"
legal_countries = merged[merged["Legal Status"].str.startswith("Legal")]

# Join all the remaining text for the selected countries into a single string
text = ' '.join(legal_countries['Remaining Text'])

# Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text)

# Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Import necessary libraries
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])

# filter the DataFrame to include only the rows where the legal status starts with "Legal"
illegal_countries = merged[merged["Legal Status"]=='Illegal']

# Join all the remaining text for the selected countries into a single string
text1 = ' '.join(illegal_countries['Remaining Text'])

# Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed

# Create a WordCloud object
wordcloud1 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text1)

# Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud1, interpolation='bilinear')
plt.axis('off')
plt.show()

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt



# Filter for the countries with legal status not starting with "Legal" and not "Illegal"
legal_status_mask = ~merged['Legal Status'].str.startswith('Legal', na=False) & (merged['Legal Status'] != 'Illegal')
remaining_text = merged.loc[legal_status_mask, 'Remaining Text'].dropna()

# Join all remaining text into a single string
text2 = ' '.join(remaining_text)

# Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed

# Create a WordCloud object
wordcloud2 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text2)

# Plot the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud2)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

# Import necessary libraries
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=['Article Contents (EN)'])

# filter the DataFrame to include only the rows where the legal status starts with "Legal"
legal_countries1 = merged[merged["Legal Status"].str.startswith("Legal")]

# Join all the remaining text for the selected countries into a single string
text3 = ' '.join(legal_countries1['Article Contents (EN)'])

# Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed

# Create a WordCloud object
wordcloud3 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text3)

# Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud3, interpolation='bilinear')
plt.axis('off')
plt.show()

# Import necessary libraries
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=['Article Contents (EN)'])

# filter the DataFrame to include only the rows where the legal status starts with "Legal"
illegal_countries1 = merged[merged["Legal Status"]=='Illegal']

# Join all the remaining text for the selected countries into a single string
text4 = ' '.join(illegal_countries1['Article Contents (EN)'])

# Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed

# Create a WordCloud object
wordcloud4 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text4)

# Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud4, interpolation='bilinear')
plt.axis('off')
plt.show()

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt



# Filter for the countries with legal status not starting with "Legal" and not "Illegal"
legal_status_mask1 = ~merged['Legal Status'].str.startswith('Legal', na=False) & (merged['Legal Status'] != 'Illegal')
remaining_text1 = merged.loc[legal_status_mask1, 'Article Contents (EN)'].dropna()

# Join all remaining text into a single string
text5 = ' '.join(remaining_text1)

# Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed

# Create a WordCloud object
wordcloud5 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text5)

# Plot the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud5)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""# Topic Modeling"""

#Import necessary libraries
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('all')
nltk.download('stopwords')
import pandas as pd
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2022)

#Pre-process text function
def preprocess(text):
    #Tokenize the text
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(text)
    #Remove stopwords
    stopword_list = set(stopwords.words("english"))
    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]
    #Lemmatize the filtered tokens
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    #Return the lemmatized tokens as a string
    return " ".join(lemmatized_tokens)

#Check if any row in "Remaining Text" column contains NaN values
if merged["Remaining Text"].isnull().sum() > 0:
    #Drop rows with NaN values in "Remaining Text" column
    merged.dropna(subset=["Remaining Text"], inplace=True)

#Preprocess the text data in "Remaining Text" column
processed_docs1 = merged["Remaining Text"].apply(preprocess)

#Convert processed_docs to a list of lists:
processed_docs1 = [doc1.split() for doc1 in processed_docs1]

#Build a dictionary representation of the documents
dictionary1 = gensim.corpora.Dictionary(processed_docs1)
#Filter out the extremes (similar to the min/max df step used when creating the TF-IDF matrix)
dictionary1.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)
#Bag-of-words representation of the documents
bow_corpus1 = [dictionary1.doc2bow(doc1) for doc1 in processed_docs1]

#Train a LDA model
num_topics = 10
lda_model1 = gensim.models.LdaMulticore(bow_corpus1, num_topics=num_topics, id2word=dictionary1)

#Get the most dominant topic for each row
topic_weights1 = []
for row1 in lda_model1[bow_corpus1]:
    row_weights1 = [w1[1] for w1 in row1]
    topic_weights1.append(row_weights1)

dominant_topic1 = [np.argmax(weights1) for weights1 in topic_weights1]

#Get the topic words for each dominant topic
topic_words1 = []
for topic_id1 in range(num_topics):
    top_words1 = [(word1, prob1) for word1, prob1 in lda_model1.show_topic(topic_id1, topn=10)]
    topic_words1.append(top_words1)

#Reset the indices of the DataFrame
merged.reset_index(inplace=True, drop=True)

#Append the dominant topic and topic words to the DataFrame
merged["Dominant_Topic"] = dominant_topic1
merged["Topic_Words"] = [topic_words1[idx1] for idx1 in dominant_topic1]

#Print the DataFrame with the new columns
display(merged)

#Check if any row in "Article Contents" column contains NaN values
if merged["Article Contents (EN)"].isnull().sum() > 0:
    #Drop rows with NaN values in "Article Contents" column
    merged.dropna(subset=["Article Contents (EN)"], inplace=True)

#Preprocess the text data in "Article Contents" column
processed_docs2 = merged["Article Contents (EN)"].apply(preprocess)

#Convert processed_docs to a list of lists:
processed_docs2 = [doc2.split() for doc2 in processed_docs2]

#Build a dictionary representation of the documents
dictionary2 = gensim.corpora.Dictionary(processed_docs2)
#Filter out the extremes (similar to the min/max df step used when creating the TF-IDF matrix)
dictionary2.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)
#Bag-of-words representation of the documents
bow_corpus2 = [dictionary2.doc2bow(doc2) for doc2 in processed_docs2]

#Train a LDA model
num_topics = 10
lda_model2 = gensim.models.LdaMulticore(bow_corpus2, num_topics=num_topics, id2word=dictionary2)

#Get the most dominant topic for each row
topic_weights2 = []
for row2 in lda_model2[bow_corpus2]:
    row_weights2 = [w2[1] for w2 in row2]
    topic_weights2.append(row_weights2)

dominant_topic2 = [np.argmax(weights2) for weights2 in topic_weights2]

#Get the topic words for each dominant topic
topic_words2 = []
for topic_id2 in range(num_topics):
    top_words2 = [(word2, prob2) for word2, prob2 in lda_model2.show_topic(topic_id2, topn=10)]
    topic_words2.append(top_words2)

#Reset the indices of the DataFrame
merged.reset_index(inplace=True, drop=True)

#Append the dominant topic and topic words to the DataFrame
merged["Dominant_Topic2"] = dominant_topic2
merged["Topic_Words2"] = [topic_words2[idx2] for idx2 in dominant_topic2]

#Print the DataFrame with the new columns
display(merged)

merged.to_csv(r'C:\Users\wuxiong\Downloads\research.csv')

# Concatenate all the top words from each topic
top_words = []
for i in range(num_topics):
    top_words.extend([word for word, prob in topic_words1[i][:5]])

# Combine similar words into the same group
word_groups = {'cryptocurrency': ['cryptocurrency', 'cryptocurrencies','crypto'],
               'market': ['market', 'markets'],
               'company': ['company', 'companies'],
               'investor': ['investor', 'investors'],
               'price': ['price', 'prices'],
               'bitcoin': ['bitcoin', 'bitcoins'],
               'blockchain': ['blockchain', 'blockchains']}

# Count the occurrence of each word and group
word_counts = {}
for word in top_words:
    for group in word_groups.keys():
        if word in word_groups[group]:
            word = group
    if word in word_counts:
        word_counts[word] += 1
    else:
        word_counts[word] = 1

# Create the pie chart
fig, ax = plt.subplots(figsize=(8, 8))
ax.pie(word_counts.values(), labels=word_counts.keys(), autopct='%1.1f%%')
ax.set_title('Distribution of Top Words')

plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Create a dictionary to map similar words to a combined category
# Combine similar words into the same group
word_groups = {'cryptocurrency': ['cryptocurrency', 'cryptocurrencies','crypto'],
               'market': ['market', 'markets'],
               'company': ['company', 'companies'],
               'investor': ['investor', 'investors'],
               'price': ['price', 'prices'],
               'bitcoin': ['bitcoin', 'bitcoins'],
               'blockchain': ['blockchain', 'blockchains']}

# Create a new column with the combined categories
merged['Combined_Topic_Words2'] = merged['Topic_Words2'].apply(lambda x: [word_groups[word][0] if word in word_groups else word for word, _ in x])

# Get the counts of each category
word_counts2 = pd.Series([word for words in merged['Combined_Topic_Words2'] for word in words]).value_counts()

fig, ax = plt.subplots(figsize=(8, 8))
ax.pie(word_counts2, labels=word_counts2.index, autopct='%1.1f%%')
ax.set_title('Distribution of Top Words 2')

plt.show()