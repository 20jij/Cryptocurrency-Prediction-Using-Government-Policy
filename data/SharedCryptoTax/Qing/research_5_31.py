# -*- coding: utf-8 -*-
"""Research 5.31

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAcVDu2qtzIg_th0PhQutPtZE0NPSSUU

# Web Scraping
"""

# Commented out IPython magic to ensure Python compatibility.
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

######1. Web scraping the wiki page
#Make a request to the webpage
url = 'https://en.wikipedia.org/wiki/Legality_of_cryptocurrency_by_country_or_territory'
response = requests.get(url)
#Parse the HTML content
soup = BeautifulSoup(response.content, 'html.parser')
#Find all the tables in the webpage
tables = soup.find_all('table', {'class': 'wikitable'})
#Initialize an empty list to store the data
data = []
#Iterate through the tables
for table in tables:
    #Iterate through the rows of the table
    for row in table.find_all('tr'):
        cells = row.find_all('td')
        if len(cells) > 0:
            #Extract the information from each cell
            country = cells[0].text.strip()
            status = cells[1].text.strip()
            #Split the legal status by '\n'
            status_list = status.split('\n')
            #Get the legal status
            legal_status = status_list[0]
            #Get the text
            remaining_text = " ".join(status_list[1:])
            #Extract the date from the text
            date_match = re.search(r'(\d{1,2}\s[A-Za-z]+\s\d{4}|\d{4}|\d{1,2}\s[A-Za-z]+\s\d{2}|\b[A-Za-z]+\s\d{4}\b)', remaining_text)
            if date_match:
                date = date_match.group()
            else:
                date = ""
            data.append([country, legal_status, remaining_text, date])
#Create a pandas DataFrame from the list
df = pd.DataFrame(data, columns=['Country', 'Legal Status','Remaining Text', 'Date In Text'])
#Get the reference number in the remaining text
reference_number = []
for i in range(len(df)):
    num  = ' '.join(set(re.findall(r"\[(\d+)\]",df['Remaining Text'][i])))
    reference_number.append(num)
df['Reference number'] = reference_number
#Reset the index
df1 = df.set_index(['Country', 'Legal Status','Remaining Text', 'Date In Text']).apply(lambda x: x.str.split(' ').explode()).reset_index()

#Web scraping the citation number, citation link, and citation date from the citation section at the bottom of the wiki page
ref_section = soup.find_all("ol", class_="references")
references = ref_section[1].find_all('li')
#Save the reference id and reference link
id = []
links = []
dates = []
for i in range(0,len(references)):
  id.append(references[i].get('id')) #Get the citation id
  text = references[i].text
  ref_dates = []
  #Get the date in citation(time has different formats)
  date_match = re.findall(r'(?P<day>\d+)\s+(?P<month>\w+)\s+(?P<year>\d{4})', text)
  if date_match:
    for match in date_match:
        day = match[0]
        month = match[1]
        year = match[2]
        ref_dates.append(f"{day} {month} {year}")
  else:
    date_match = re.findall(r'(?P<month>\w+)\s+(?P<year>\d{4})', text)
    if date_match:
        for match in date_match:
            month = match[0]
            year = match[1]
            ref_dates.append(f"{month} {year}")
  if len(ref_dates) > 0:
    dates.append(ref_dates)
  else:
    dates.append("NA")
  #Get the citation link
  try:
    links.append(references[i].find_all("a", class_="external text")[0].get('href')) #Get the citation link when class is external text
  except:
    links.append(references[i].find_all("a", class_="external free")[0].get('href')) #Get the citation link when class is external free
#Create a DataFrame from the list
df2 = pd.DataFrame({'Citation Id':id, 'Citation Link':links, 'Citation Date':dates })
#Get the citation number
citation_number = []
for i in range(len(df2)):
  id = re.findall(r'\b\d+\b', df2['Citation Id'][i])[-1]
  citation_number.append(id)
df2['Citation number'] = citation_number

#Merged the web scraping dataframe with the reference dataframe
merged = pd.merge(df1, df2, left_on='Reference number', right_on='Citation number', how ="outer")
merged.sort_values(by='Country', ascending=True, inplace=True)
merged.reset_index(drop=True, inplace=True)

# Extract only the first element before the comma in the citation date(want to get the earliest date if there are two date information)
merged['Citation Date'] = merged['Citation Date'].apply(lambda x: x[0].split(',')[0] if isinstance(x, list) and len(x) > 0 else '')

#Ensure all the date is in the format: yyyy-mm-dd
merged['Date In Text'] = pd.to_datetime(merged['Date In Text'], errors='coerce').dt.strftime('%Y-%m-%d')
merged['Citation Date'] = pd.to_datetime(merged['Citation Date'], errors='coerce').dt.strftime('%Y-%m-%d')



######2. Web scraping the article contents

#Web scraping all the article contents
# %pip install newspaper3k
#Using newspaper3k to extract the articles of the links
from newspaper import Article
#Article extraction function
def extract_article_contents(url):
    article = Article(url)
    article.download()
    article.parse()
    return article.text
#Append the article contents to the dataframe
article_contents = []
for link in merged['Citation Link']:
    try:
      article_contents.append(extract_article_contents(link))
    except:
      try:
        from newspaper import Config
        user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
        config = Config()
        config.browser_user_agent = user_agent 
        article = Article(link, config=config)
        article.download()
        article.parse()
        article_contents.append(article.text)
      except:
        article_contents.append("No Link")
#Put article contents into the dataframe
merged['Article Contents'] = article_contents
display(merged)

"""# Translate the article contents"""

#Not work so well with all the translation
!pip install googletrans==4.0.0-rc1

from googletrans import Translator

#Create a translator object
translator = Translator()

#Translate all the articles
article_contents_en = []
for article in merged['Article Contents']:
    try:
        article_en = translator.translate(article, dest='en').text
        article_contents_en.append(article_en)
    except:
        article_contents_en.append("Translation Unavailable")
#Add the translated articles to the DataFrame
merged['Article Contents (EN)'] = article_contents_en
display(merged)

"""# Display rows with empty article contents and translation is unavailable




"""

#Rows that do not scrape the article contents correctly
#Create a boolean mask to identify rows with empty or NA "Article Contents"
mask = merged['Article Contents'].isna() | (merged['Article Contents'] == '')
#Filter the DataFrame using the boolean mask
missing_articles = merged[mask]
#Display the filtered DataFrame
display(missing_articles)

#Rows that do not translate the article contents correctly
unavailable_rows = merged[merged['Article Contents (EN)'] == 'Translation Unavailable']
display(unavailable_rows)

"""# Word Cloud


1.   First three wordclouds for wiki page
2.   Last three wordclouds for article contents



"""

#Import necessary libraries
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

#Legal Status: Legal

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
legal_countries = merged[merged["Legal Status"].str.startswith("Legal")]
#Join all the remaining text for the selected countries into a single string
text = ' '.join(legal_countries['Remaining Text'])
#Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed
#Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text)
#Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#Legal Status: Illegal

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
illegal_countries = merged[merged["Legal Status"]=='Illegal']
#Join all the remaining text for the selected countries into a single string
text1 = ' '.join(illegal_countries['Remaining Text'])
#Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed
#Create a WordCloud object
wordcloud1 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text1)
#Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud1, interpolation='bilinear')
plt.axis('off')
plt.show()

#Legal Status: Others

#Filter for the countries with legal status not starting with "Legal" and not "Illegal"
legal_status_mask = merged[(merged['Legal Status'] != 'Legal') & (merged['Legal Status'] != 'Illegal')]
print(legal_status_mask.shape)
#Join all remaining text into a single string
text2 = ' '.join(legal_status_mask['Remaining Text'])
#Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed
#Create a WordCloud object
wordcloud2 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text2)
#Plot the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud2)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

#Legal Status: Legal

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=['Article Contents (EN)'])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
legal_countries1 = merged[merged["Legal Status"].str.startswith("Legal")]
#Join all the remaining text for the selected countries into a single string
text3 = ' '.join(legal_countries1['Article Contents (EN)'])
#Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed
#Create a WordCloud object
wordcloud3 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text3)
#Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud3, interpolation='bilinear')
plt.axis('off')
plt.show()

#Legal Status: Illegal

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=['Article Contents (EN)'])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
illegal_countries1 = merged[merged["Legal Status"]=='Illegal']
#Join all the remaining text for the selected countries into a single string
text4 = ' '.join(illegal_countries1['Article Contents (EN)'])
#Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed
#Create a WordCloud object
wordcloud4 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text4)
#Display the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud4, interpolation='bilinear')
plt.axis('off')
plt.show()

#Legal Status: Others

#Filter for the countries with legal status not starting with "Legal" and not "Illegal"
legal_status_mask1 = ~merged['Legal Status'].str.startswith('Legal', na=False) & (merged['Legal Status'] != 'Illegal')
remaining_text1 = merged.loc[legal_status_mask1, 'Article Contents (EN)'].dropna()
#Join all remaining text into a single string
text5 = ' '.join(remaining_text1)
#Create a set of stopwords to exclude from the wordcloud
stopwords = set(STOPWORDS)
stopwords.update(["one", "two", "three"])  # add additional stopwords as needed
#Create a WordCloud object
wordcloud5 = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text5)
#Plot the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud5)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""# Graphs of the relationship between countries, legal status, bitcoin price"""

import matplotlib.pyplot as plt

#Read the Excel file
b_price = pd.read_excel('/price.xlsx')
#Convert the "Date" column to datetime format
b_price['Date'] = pd.to_datetime(b_price['Date'])
#Set up the figure and axis
fig, ax = plt.subplots(figsize=(12, 8))
#Filter the relevant columns from the merged DataFrame
timeline_data = merged[['Country', 'Date In Text', 'Legal Status']]
#Drop rows with missing or invalid dates
timeline_data = timeline_data.dropna(subset=['Date In Text'])
#Convert the 'Date In Text' column to datetime format
timeline_data['Date In Text'] = pd.to_datetime(timeline_data['Date In Text'], errors='coerce')
#Create a dictionary to map legal statuses to colors
status_colors = {'Legal': 'green', 'Illegal': 'red', 'Others': 'blue'}
#Assign a common y-value to all legal statuses
y_value = 1

#Iterate over each country
for country in timeline_data['Country'].unique():
    #Filter data for the current country
    country_data = timeline_data[timeline_data['Country'] == country]
    #Iterate over each legal status
    for status in status_colors.keys():
        #Filter data for the current legal status
        status_data = country_data[country_data['Legal Status'] == status]
        #Plot a scatter point for each legal status with the corresponding color and adjusted y-value
        ax.scatter(status_data['Date In Text'], [y_value] * len(status_data), color=status_colors[status], label=status, alpha=0.7)

#Set the y-axis ticks and labels
ax.set_yticks([y_value])
ax.set_yticklabels(['Status'])
#Set the x-axis label
ax.set_xlabel('Time')
#Plot the "Price" column against the "Date" column
ax2 = ax.twinx()
ax2.plot(b_price['Date'], b_price['Price'], color='black', linestyle='-', label='Price')
ax2.set_ylabel('Price')
#Add a legend for the new line
ax2.legend(loc='upper right')

#Display the plot
plt.tight_layout()
plt.show()

#Convert the "Date" column to datetime format
b_price['Date'] = pd.to_datetime(b_price['Date'])
#Filter the relevant columns from the merged DataFrame
timeline_data = merged[['Country', 'Date In Text', 'Legal Status']]
#Drop rows with missing or invalid dates
timeline_data = timeline_data.dropna(subset=['Date In Text'])
#Convert the 'Date In Text' column to datetime format
timeline_data['Date In Text'] = pd.to_datetime(timeline_data['Date In Text'], errors='coerce')
#Create a dictionary to map legal statuses to colors
status_colors = {'Legal': 'green', 'Illegal': 'red', 'Others': 'blue'}
#Iterate over each year
for year in range(2013, 2024):
    #Filter data for the current year
    year_data = timeline_data[timeline_data['Date In Text'].dt.year == year] 
    #Set up the figure and axis for the current year
    fig, ax = plt.subplots(figsize=(12, 8))
    #Assign a common y-value to all legal statuses
    y_value = 1 
    #Iterate over each country
    for country in year_data['Country'].unique():
        #Filter data for the current country
        country_data = year_data[year_data['Country'] == country]
        #Iterate over each legal status
        for status in status_colors.keys():
            #Filter data for the current legal status
            status_data = country_data[country_data['Legal Status'] == status]
            #Plot a scatter point for each legal status with the corresponding color and adjusted y-value
            ax.scatter(status_data['Date In Text'], [y_value] * len(status_data), color=status_colors[status], label=status, alpha=0.7)
    
    #Set the y-axis ticks and labels
    ax.set_yticks([y_value])
    ax.set_yticklabels(['Status'])
    
    #Set the x-axis limits to January 1 to December 31 of the current year
    ax.set_xlim(pd.Timestamp(year, 1, 1), pd.Timestamp(year, 12, 31))
    
    #Set the x-axis label
    ax.set_xlabel('Time')
    
    #Plot the "Price" column against the "Date" column
    ax2 = ax.twinx()
    ax2.plot(b_price['Date'], b_price['Price'], color='black', linestyle='-', label='Price')
    ax2.set_ylabel('Price')
    
    #Add a legend for the new line
    ax2.legend(loc='upper right')
    
    #Set the title for the current year
    ax.set_title('Legal Status of Cryptocurrency by Country - {}'.format(year))
    
    #Display the plot for the current year
    plt.tight_layout()
    plt.show()

"""# Topic Modeling"""

#Import necessary libraries
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('all')
nltk.download('stopwords')
import pandas as pd
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2022)

#Pre-process text function
def preprocess(text):
    #Tokenize the text
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(text)
    #Remove stopwords
    stopword_list = set(stopwords.words("english"))
    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]
    #Lemmatize the filtered tokens
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    #Return the lemmatized tokens as a string
    return " ".join(lemmatized_tokens)

###Topic Modeling for wiki page

#Check if any row in "Remaining Text" column contains NaN values
if merged["Remaining Text"].isnull().sum() > 0:
    #Drop rows with NaN values in "Remaining Text" column
    merged.dropna(subset=["Remaining Text"], inplace=True)

#Preprocess the text data in "Remaining Text" column
processed_docs1 = merged["Remaining Text"].apply(preprocess)

#Convert processed_docs to a list of lists:
processed_docs1 = [doc1.split() for doc1 in processed_docs1]

#Build a dictionary representation of the documents
dictionary1 = gensim.corpora.Dictionary(processed_docs1)
#Filter out the extremes (similar to the min/max df step used when creating the TF-IDF matrix)
dictionary1.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)
#Bag-of-words representation of the documents
bow_corpus1 = [dictionary1.doc2bow(doc1) for doc1 in processed_docs1]

#Train a LDA model
num_topics = 10
lda_model1 = gensim.models.LdaMulticore(bow_corpus1, num_topics=num_topics, id2word=dictionary1)

#Get the most dominant topic for each row
topic_weights1 = []
for row1 in lda_model1[bow_corpus1]:
    row_weights1 = [w1[1] for w1 in row1]
    topic_weights1.append(row_weights1)

dominant_topic1 = [np.argmax(weights1) for weights1 in topic_weights1]

#Get the topic words for each dominant topic
topic_words1 = []
for topic_id1 in range(num_topics):
    top_words1 = [(word1, prob1) for word1, prob1 in lda_model1.show_topic(topic_id1, topn=10)]
    topic_words1.append(top_words1)

#Reset the indices of the DataFrame
merged.reset_index(inplace=True, drop=True)

#Append the dominant topic and topic words to the DataFrame
merged["Dominant_Topic"] = dominant_topic1
merged["Topic_Words"] = [topic_words1[idx1] for idx1 in dominant_topic1]

#Print the DataFrame with the new columns
display(merged)

###Topic Modeling for article contents after translation

#Check if any row in "Article Contents" column contains NaN values
if merged["Article Contents (EN)"].isnull().sum() > 0:
    #Drop rows with NaN values in "Article Contents" column
    merged.dropna(subset=["Article Contents (EN)"], inplace=True)

#Preprocess the text data in "Article Contents" column
processed_docs2 = merged["Article Contents (EN)"].apply(preprocess)

#Convert processed_docs to a list of lists:
processed_docs2 = [doc2.split() for doc2 in processed_docs2]

#Build a dictionary representation of the documents
dictionary2 = gensim.corpora.Dictionary(processed_docs2)
#Filter out the extremes (similar to the min/max df step used when creating the TF-IDF matrix)
dictionary2.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)
#Bag-of-words representation of the documents
bow_corpus2 = [dictionary2.doc2bow(doc2) for doc2 in processed_docs2]

#Train a LDA model
num_topics = 10
lda_model2 = gensim.models.LdaMulticore(bow_corpus2, num_topics=num_topics, id2word=dictionary2)

#Get the most dominant topic for each row
topic_weights2 = []
for row2 in lda_model2[bow_corpus2]:
    row_weights2 = [w2[1] for w2 in row2]
    topic_weights2.append(row_weights2)

dominant_topic2 = [np.argmax(weights2) for weights2 in topic_weights2]

#Get the topic words for each dominant topic
topic_words2 = []
for topic_id2 in range(num_topics):
    top_words2 = [(word2, prob2) for word2, prob2 in lda_model2.show_topic(topic_id2, topn=10)]
    topic_words2.append(top_words2)

#Reset the indices of the DataFrame
merged.reset_index(inplace=True, drop=True)

#Append the dominant topic and topic words to the DataFrame
merged["Dominant_Topic2"] = dominant_topic2
merged["Topic_Words2"] = [topic_words2[idx2] for idx2 in dominant_topic2]

#Print the DataFrame with the new columns
display(merged)

###Topic Modeling for article contetns without translation

#Check if any row in "Article Contents" column contains NaN values
if merged["Article Contents"].isnull().sum() > 0:
    #Drop rows with NaN values in "Article Contents" column
    merged.dropna(subset=["Article Contents"], inplace=True)

#Preprocess the text data in "Article Contents" column
processed_docs3 = merged["Article Contents"].apply(preprocess)

#Convert processed_docs to a list of lists:
processed_docs3 = [doc3.split() for doc3 in processed_docs3]

#Build a dictionary representation of the documents
dictionary3 = gensim.corpora.Dictionary(processed_docs3)
#Filter out the extremes (similar to the min/max df step used when creating the TF-IDF matrix)
dictionary3.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)
#Bag-of-words representation of the documents
bow_corpus3 = [dictionary3.doc2bow(doc3) for doc3 in processed_docs3]

#Train a LDA model
num_topics = 10
lda_model3 = gensim.models.LdaMulticore(bow_corpus3, num_topics=num_topics, id2word=dictionary3)

#Get the most dominant topic for each row
topic_weights3 = []
for row3 in lda_model3[bow_corpus3]:
    row_weights3 = [w3[1] for w3 in row3]
    topic_weights3.append(row_weights3)

dominant_topic3 = [np.argmax(weights3) for weights3 in topic_weights3]

#Get the topic words for each dominant topic
topic_words3 = []
for topic_id3 in range(num_topics):
    top_words3 = [(word3, prob3) for word3, prob3 in lda_model3.show_topic(topic_id3, topn=10)]
    topic_words3.append(top_words3)

#Reset the indices of the DataFrame
merged.reset_index(inplace=True, drop=True)

#Append the dominant topic and topic words to the DataFrame
merged["Dominant_Topic3"] = dominant_topic3
merged["Topic_Words3"] = [topic_words3[idx3] for idx3 in dominant_topic3]

#Print the DataFrame with the new columns
display(merged)

"""# Graphs for Topic Modeling Results"""

merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
legal_countries = merged[merged["Legal Status"].str.startswith("Legal")]
legal_countries["Topic_Words"] = [max(topic, key=lambda x: x[1])[0] for topic in legal_countries["Topic_Words"]]
legal_countries["Topic_Words"]

import nltk
import matplotlib.pyplot as plt
import seaborn as sns

###Topic Modeling for wiki: Legal
#Step 1: Create a list of all the words in the legal_df["Topic_Words"] column
all_words = []
for words in legal_countries["Topic_Words"]:
    all_words.append(words)
#Create a Pandas Series with the frequency of each topic word
word_counts = pd.Series(all_words).value_counts()
#Create a bar plot
sns.set(style="darkgrid")
plt.figure(figsize=(8,4))
sns.barplot(x=word_counts.index, y=word_counts.values, color="blue")
plt.title("Distribution of Topic Words", fontsize=16)
plt.xlabel("Topic Words", fontsize=14)
plt.ylabel("Frequency", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.show()

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
illegal_countries = merged[merged["Legal Status"].str.startswith("Illegal")]
illegal_countries["Topic_Words"] = [max(topic, key=lambda x: x[1])[0] for topic in illegal_countries["Topic_Words"]]
illegal_countries["Topic_Words"]

###Topic Modeling for wiki: Illegal
#Step 1: Create a list of all the words in the legal_df["Topic_Words"] column
all_words = []
for words in illegal_countries["Topic_Words"]:
    all_words.append(words)
#Create a Pandas Series with the frequency of each topic word
word_counts = pd.Series(all_words).value_counts()
#Create a bar plot
sns.set(style="darkgrid")
plt.figure(figsize=(8,4))
sns.barplot(x=word_counts.index, y=word_counts.values, color="blue")
plt.title("Distribution of Topic Words", fontsize=16)
plt.xlabel("Topic Words", fontsize=14)
plt.ylabel("Frequency", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.show()

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
legal_status_mask = merged[(merged['Legal Status'] != 'Legal') & (merged['Legal Status'] != 'Illegal')]
legal_status_mask["Topic_Words"] = [max(topic, key=lambda x: x[1])[0] for topic in legal_status_mask["Topic_Words"]]
legal_status_mask["Topic_Words"]

###Topic Modeling for wiki: Others
#Step 1: Create a list of all the words in the legal_df["Topic_Words"] column
all_words = []
for words in legal_status_mask["Topic_Words"]:
    all_words.append(words)
#Create a Pandas Series with the frequency of each topic word
word_counts = pd.Series(all_words).value_counts()
#Create a bar plot
sns.set(style="darkgrid")
plt.figure(figsize=(8,4))
sns.barplot(x=word_counts.index, y=word_counts.values, color="blue")
plt.title("Distribution of Topic Words", fontsize=16)
plt.xlabel("Topic Words", fontsize=14)
plt.ylabel("Frequency", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.show()

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
legal_countries1 = merged[merged["Legal Status"].str.startswith("Legal")]
legal_countries1["Topic_Words2"] = [max(topic, key=lambda x: x[1])[0] for topic in legal_countries1["Topic_Words2"]]
legal_countries1["Topic_Words2"]

###Topic Modeling for articles: Legal
#Step 1: Create a list of all the words in the legal_df["Topic_Words"] column
all_words1 = []
for words in legal_countries1["Topic_Words2"]:
    all_words1.append(words)
#Create a Pandas Series with the frequency of each topic word
word_counts1 = pd.Series(all_words1).value_counts()
#Create a bar plot
sns.set(style="darkgrid")
plt.figure(figsize=(8,4))
sns.barplot(x=word_counts1.index, y=word_counts1.values, color="blue")
plt.title("Distribution of Topic Words 2", fontsize=16)
plt.xlabel("Topic Words 2", fontsize=14)
plt.ylabel("Frequency 2", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.show()

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
illegal_countries1 = merged[merged["Legal Status"].str.startswith("Illegal")]
illegal_countries1["Topic_Words2"] = [max(topic, key=lambda x: x[1])[0] for topic in illegal_countries1["Topic_Words2"]]
illegal_countries1["Topic_Words2"]

###Topic Modeling for articles: Illegal
#Step 1: Create a list of all the words in the legal_df["Topic_Words"] column
all_words2 = []
for words in illegal_countries1["Topic_Words2"]:
    all_words2.append(words)
#Create a Pandas Series with the frequency of each topic word
word_counts2 = pd.Series(all_words2).value_counts()
#Create a bar plot
sns.set(style="darkgrid")
plt.figure(figsize=(8,4))
sns.barplot(x=word_counts2.index, y=word_counts2.values, color="blue")
plt.title("Distribution of Topic Words 2", fontsize=16)
plt.xlabel("Topic Words 2", fontsize=14)
plt.ylabel("Frequency 2", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.show()

#Filter the data to include only countries with legal status starting with "Legal"
merged = merged.dropna(subset=["Remaining Text"])
#Filter the DataFrame to include only the rows where the legal status starts with "Legal"
other = merged[(merged['Legal Status'] != 'Legal') & (merged['Legal Status'] != 'Illegal')]
other["Topic_Words2"] = [max(topic, key=lambda x: x[1])[0] for topic in other["Topic_Words2"]]
other["Topic_Words2"]

###Topic Modeling for articles: Others
#Step 1: Create a list of all the words in the legal_df["Topic_Words"] column
all_words3 = []
for words in other["Topic_Words2"]:
    all_words3.append(words)
#Create a Pandas Series with the frequency of each topic word
word_counts3 = pd.Series(all_words3).value_counts()
#Create a bar plot
sns.set(style="darkgrid")
plt.figure(figsize=(8,4))
sns.barplot(x=word_counts3.index, y=word_counts3.values, color="blue")
plt.title("Distribution of Topic Words 2", fontsize=16)
plt.xlabel("Topic Words 2", fontsize=14)
plt.ylabel("Frequency 2", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.show()